from fastapi import APIRouter, Depends, HTTPException, Header, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List, Optional, Literal
from pydantic import BaseModel, Field, field_validator
from datetime import datetime
import logging
import httpx
import json

from app.core.database import (
    get_db, User, APIKey, UserQuota, 
    get_or_create_quota, SubscriptionTier, QUOTA_LIMITS
)
from app.services.memory_core.graph_memory_service import graph_memory_service
from app.services.memory_queue import add_memory_task, get_queue_for_tier
from app.core.celery_config import celery_app
from app.core.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()

router = APIRouter(prefix="/v1", tags=["conversations"])


class MessageItem(BaseModel):
    role: Literal["user", "assistant"] = Field(..., description="消息角色：user 或 assistant")
    content: str = Field(..., min_length=1, description="消息内容")
    tokens: Optional[int] = Field(default=0, description="Token 数量")
    timestamp: Optional[int] = Field(default=None, description="时间戳")


class ConversationFlushRequest(BaseModel):
    conversation_id: Optional[str] = Field(default=None, description="对话ID")
    messages: List[MessageItem] = Field(..., min_length=1, description="消息列表，每条包含 role、content、timestamp")


class SensitiveFilterResult(BaseModel):
    has_sensitive: bool
    filtered_content: str
    sensitive_count: int


SENSITIVE_FILTER_PROMPT = """请将以下对话中的敏感信息替换为[已过滤]：

必须替换以下类型：
1. 银行卡卡号：任何长度的银行卡号（不限制位数，如62220000111122223333、4532123456789012等）
2. 密码：密码后面跟的内容（如"密码是abc123"替换为"密码是[已过滤]"）
3. 身份证号码：如110101199001011234、420123198512125678等18位中国身份证号
4. 社保号码：如123-45-6789等社会保险号码
5. 护照号码：如GB1234567、E12345678等护照号码
6. 驾驶证号码：驾驶证编号

不要替换以下内容：
- 姓名（如张三、李四、John）
- 地址（如北京市朝阳区、New York）
- 手机号码（如13812345678、+1234567890）
- 电子邮箱

原始内容：
{conversation}

请严格按以下 JSON 格式返回：
{{"has_sensitive": true或false, "safe_content": "替换敏感信息后的内容"}}"""


async def filter_sensitive_with_llm(content: str) -> SensitiveFilterResult:
    """使用 LLM 过滤敏感信息 - OpenAI 兼容 API"""
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{settings.ollama_base_url}/v1/chat/completions",
                headers={"Content-Type": "application/json"},
                json={
                    "model": settings.llm_model,
                    "messages": [
                        {
                            "role": "system",
                            "content": "你是一个敏感信息识别助手。请分析对话内容，识别并替换所有敏感信息。只返回JSON格式结果。"
                        },
                        {
                            "role": "user",
                            "content": SENSITIVE_FILTER_PROMPT.format(conversation=content)
                        }
                    ],
                    "temperature": 0.1,
                    "response_format": {"type": "json_object"}
                }
            )
            
            if response.status_code != 200:
                logger.error(f"LLM filter failed: {response.status_code} - {response.text}")
                return SensitiveFilterResult(
                    has_sensitive=False,
                    filtered_content=content,
                    sensitive_count=0
                )
            
            data = response.json()
            llm_response = data.get("choices", [{}])[0].get("message", {}).get("content", "{}")
            
            result = json.loads(llm_response)
            safe_content = result.get("safe_content", content)
            sensitive_count = safe_content.count("[已过滤]") if safe_content else 0
            
            return SensitiveFilterResult(
                has_sensitive=result.get("has_sensitive", False),
                filtered_content=safe_content,
                sensitive_count=sensitive_count
            )
            
    except json.JSONDecodeError as e:
        logger.error(f"LLM response parse failed: {e}")
        return SensitiveFilterResult(
            has_sensitive=False,
            filtered_content=content,
            sensitive_count=0
        )
    except Exception as e:
        logger.error(f"LLM filter error: {e}")
        return SensitiveFilterResult(
            has_sensitive=False,
            filtered_content=content,
            sensitive_count=0
        )


def process_conversation_task(conversation_data: dict, api_key_id: int = None):
    """后台任务：处理对话提取记忆 - 通过队列异步处理"""
    try:
        user_id = conversation_data["user_id"]
        messages = conversation_data["messages"]
        
        from app.core.database import SessionLocal, User
        db = SessionLocal()
        try:
            user = db.query(User).filter(User.id == int(user_id)).first()
            tier = user.subscription_tier if user else SubscriptionTier.FREE
        finally:
            db.close()
        
        import asyncio
        import json
        messages_json = json.dumps(messages, ensure_ascii=False, indent=2)
        filter_result = asyncio.run(filter_sensitive_with_llm(messages_json))
        
        if filter_result.has_sensitive:
            logger.info(f"LLM filtered {filter_result.sensitive_count} sensitive items for user {user_id}")
        
        queue = get_queue_for_tier(tier)
        task = add_memory_task.apply_async(
            args=[user_id, filter_result.filtered_content, {
                "conversation_id": conversation_data.get("conversation_id"),
                "message_count": len(messages),
                "has_sensitive": filter_result.has_sensitive,
                "source": "conversation_flush",
                "project_id": conversation_data.get("project_id", "default")
            }, False, api_key_id],
            queue=queue
        )
        
        logger.info(f"Queued memory task for user {user_id}: {task.id} in queue {queue}")
        return {"task_id": task.id, "status": "queued"}
        
    except Exception as e:
        logger.error(f"Failed to process conversation: {e}")
        raise


def get_current_user_with_quota(
    x_api_key: str = Header(None), 
    db: Session = Depends(get_db)
) -> tuple:
    if not x_api_key:
        raise HTTPException(status_code=401, detail="X-API-Key header required")
    
    api_key = db.query(APIKey).filter(
        APIKey.api_key == x_api_key, 
        APIKey.is_active == True
    ).first()
    
    if not api_key:
        raise HTTPException(status_code=401, detail="Invalid API Key")
    
    user = db.query(User).filter(User.id == api_key.user_id).first()
    if not user:
        raise HTTPException(status_code=401, detail="User not found")
    
    quota = get_or_create_quota(db, user.id)
    
    return user.id, user.subscription_tier, quota, api_key.id


@router.post("/conversations/flush")
async def flush_conversation(
    request: ConversationFlushRequest,
    background_tasks: BackgroundTasks,
    user_data: tuple = Depends(get_current_user_with_quota),
    db: Session = Depends(get_db)
):
    """
    批量提交对话 - 触发记忆提取
    
    流程：
    1. 接收 messages JSON 数组
    2. 格式校验（每条消息必须有 role、content、timestamp）
    3. 后台异步处理：敏感信息过滤 + LLM 提取记忆 + 向量化 + 图构建
    """
    user_id, tier, quota, api_key_id = user_data
    
    if not request.messages or len(request.messages) == 0:
        raise HTTPException(status_code=400, detail="messages field is required")
    
    conversation_data = {
        "user_id": str(user_id),
        "conversation_id": request.conversation_id,
        "messages": [m.model_dump() for m in request.messages],
        "project_id": "default"
    }
    
    background_tasks.add_task(process_conversation_task, conversation_data, api_key_id)
    
    db.commit()
    
    return {
        "status": "ok",
        "message": "Conversation queued for processing",
        "message_count": len(request.messages),
        "extracted_count": 1,
        "server_model_version": 1,
        "sync_required": False
    }


@router.post("/conversations/realtime")
async def realtime_message(
    message: MessageItem,
    user_data: tuple = Depends(get_current_user_with_quota),
    db: Session = Depends(get_db)
):
    """
    实时消息接收 - 立即处理
    
    用于高优先级消息，绕过缓冲直接处理
    """
    user_id, tier, quota, api_key_id = user_data
    
    if not message.content or len(message.content) < 2:
        return {"status": "skipped", "reason": "content_too_short"}
    
    filter_result = await filter_sensitive_with_llm(message.content)
    
    queue = get_queue_for_tier(tier)
    task = add_memory_task.apply_async(
        args=[str(user_id), filter_result.filtered_content, {
            "role": message.role,
            "tokens": message.tokens,
            "source": "realtime",
            "has_sensitive": filter_result.has_sensitive
        }, False, api_key_id],
        queue=queue
    )
    
    db.commit()
    
    return {
        "status": "queued",
        "task_id": task.id,
        "has_sensitive": filter_result.has_sensitive
    }
